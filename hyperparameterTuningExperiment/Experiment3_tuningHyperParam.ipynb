{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In summary, this script exemplifies a structured and comprehensive approach to hyperparameter tuning for a language model. By systematically exploring a range of configurations and rigorously evaluating model performance, it aims to identify the optimal set of hyperparameters that lead to the best predictive performance on the given language modeling task.\n",
        "\n",
        "Key Steps in Hyperparameter Tuning:\n",
        "\n",
        "1. **Configuration and Setup**: The script begins by loading configuration settings from a YAML file. These settings include paths to data, model architecture specifications, and ranges for various hyperparameters like learning rate, weight decay, and network structure (number of layers, embedding size, hidden layer size). The configuration also dictates the runtime environment and training parameters like batch size and number of epochs.\n",
        "\n",
        "2. **Data Preparation**: Using a `WikiText2VocabBuilder` class, a vocabulary is built from a text corpus. This vocabulary is then used to create training and validation datasets, which are crucial for both training the model and evaluating its performance during the tuning process.\n",
        "\n",
        "3. **Model Architecture**: The `LanguageModel` class defines the neural network's structure, including an embedding layer, multiple hidden layers (with ReLU activation and layer normalization), and an output layer. The model is designed to use residual connections and Kaiming initialization to aid in training deeper networks efficiently.\n",
        "\n",
        "4. **Hyperparameter Sampling**: The script employs two key functions - `sample_logHyperparameters` and `sample_uniformHyperparameters` - to sample hyperparameters from specified ranges. The former samples values like learning rate and weight decay from log-uniform distributions, ensuring a uniform distribution across orders of magnitude. The latter samples architectural parameters like the number of layers and sizes of the embedding and hidden layers.\n",
        "\n",
        "5. **Training and Evaluation Loop**: The core of the tuning process is the `hyper_param_tuning` function. This function orchestrates the training of the model with the sampled hyperparameters. It involves training the model for a set number of epochs, evaluating its performance on the validation set, and applying mechanisms like early stopping and learning rate scheduling. Crucially, it records metrics such as loss and perplexity, which are vital for assessing the model's performance.\n",
        "\n",
        "6. **Iterative Process**: The script executes multiple iterations of the tuning process, each time sampling a new set of hyperparameters and training a new model instance. This iterative approach is key to exploring the hyperparameter space thoroughly.\n",
        "\n",
        "7. **Logging and Analysis**: Throughout the process, important information about each hyperparameter set and the corresponding model's performance is logged. This data is saved both in log files and a JSON format, enabling detailed post-hoc analysis to determine the most effective model configuration.\n",
        "\n",
        "8. **Final Selection**: After all iterations are complete, the script identifies and reports the best-performing model configuration based on validation perplexity, a standard measure of model performance in language modeling tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "uOJ6QztZJ35i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VWswtGaJU8f"
      },
      "source": [
        "The code cell below sets up the necessary Python libraries for building and training neural network models, for tasks involving natural language processing and data visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpS9uWT8JU8l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import random\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "import string\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "import json\n",
        "import string\n",
        "import math\n",
        "import sys\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DPM-ZT7JU8p"
      },
      "source": [
        "The code reads a configuration file to set parameters for a machine learning model's data processing, runtime environment, and hyperparameter tuning. It specifies data paths, minimum word frequency, context size for training, shuffle settings, worker threads, batch size, and device type. For the experiment, it defines the number of training epochs, early stopping patience, and the number of hyperparameter tuning iterations. Hyperparameters such as learning rate, weight decay, reduction factor, number of layers, embedding, and hidden layer sizes are set as ranges to explore the best model configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXBbtnFSJU8p"
      },
      "outputs": [],
      "source": [
        "with open(\"tuningHyperParamConfig.yaml\", 'r') as ymlfile:\n",
        "    config = yaml.safe_load(ymlfile)\n",
        "\n",
        "training_data_corpus_path = config['data']['training_data_corpus_path']\n",
        "validation_data_corpus_path = config['data']['validation_data_corpus_path']\n",
        "min_freq = config['data']['min_freq']\n",
        "context_size = config['data']['context_size']\n",
        "\n",
        "shuffle = config['runtime']['shuffle']\n",
        "num_workers = config['runtime']['num_workers']\n",
        "batch_size = config['runtime']['batch_size']\n",
        "config_device = config['runtime']['device']\n",
        "\n",
        "\n",
        "total_epochs = config['experiment']['total_epochs']\n",
        "patience = config['experiment']['patience']\n",
        "lr_start_range = config['hyperparameters']['lr_start_range']\n",
        "weight_decay_range = config['hyperparameters']['weight_decay_range']\n",
        "reduction_factor_range = config['hyperparameters']['reduction_factor_range']\n",
        "num_iterations = config['experiment']['num_iterations_for_hyperparameters'] # Number of combinations of hyperparameters to try\n",
        "numberOfLayers_range = config['hyperparameters']['numberOfLayers_range'] # Number of hidden layers\n",
        "embed_size_range = config['hyperparameters']['embed_size_range']  # The embedding size of the words\n",
        "hidden_size_range = config['hyperparameters']['hidden_size_range'] # The hidden size of the neural network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM2EZfbUJU8q"
      },
      "source": [
        "The code cell below defines a function `init_logger()` that sets up logging for a hyperparameter tuning process. It generates a unique filename incorporating the current date and time, configures the logging level, message format, and date format, and begins logging to this file. When called, the function initializes the logger and prints the log file's name, indicating where the tuning process's details will be recorded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO9aldROJU8r",
        "outputId": "0fe84294-bfb3-4735-d7c4-0e00b942d822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging to file: hyperparameter_tuning_2023-09-16_13-47-27.log\n"
          ]
        }
      ],
      "source": [
        "# Function to initialize logging with dynamic filename\n",
        "def init_logger():\n",
        "    # Generate a unique log file name based on the current date and time\n",
        "    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    log_file_name = f'hyperparameter_tuning_{current_time}.log'\n",
        "\n",
        "    # Configure the logging module to write the logs to a file\n",
        "    logging.basicConfig(filename=log_file_name,\n",
        "                        level=logging.INFO,\n",
        "                        format='%(asctime)s [%(levelname)s]: %(message)s',\n",
        "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
        "    return log_file_name\n",
        "\n",
        "# Initialize logger and get the log file name\n",
        "log_file_name = init_logger()\n",
        "print(f\"Logging to file: {log_file_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LTrRgJgJU8t"
      },
      "source": [
        "Preparation of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riro0g0pJU8u"
      },
      "source": [
        "The code cell below defines a class `WikiText2VocabBuilder` which constructs a vocabulary from a text corpus. The class initializes with paths to the corpus and a frequency threshold for inclusion. It manages a bidirectional mapping of words to indices and tracks word frequencies.\n",
        "\n",
        "Special tokens for start, end, unknown words, hyphens, and numerical commas are predefined. The text is preprocessed to lowercase, tokenize sentences, and replace specific tokens with their representations. Only words meeting the minimum frequency criterion are added to the vocabulary. The `vocab_size` method returns the total number of unique words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QisMexe_JU8v"
      },
      "outputs": [],
      "source": [
        "class WikiText2VocabBuilder:\n",
        "    def _new_index(self):\n",
        "        return len(self.word2index)\n",
        "\n",
        "    def __init__(self, corpus_path, min_freq):\n",
        "        self.corpus_path = corpus_path\n",
        "        self.min_freq = min_freq\n",
        "        self.word2index = defaultdict(self._new_index)\n",
        "        self.index2word = {}\n",
        "        self.word_freqs = Counter()\n",
        "        self.cleaned_sentences = []\n",
        "\n",
        "        self.START_TOKEN = \"<s>\"\n",
        "        self.END_TOKEN = \"</s>\"\n",
        "        self.UNK_TOKEN = \"<unk>\"\n",
        "        self.hyphentoken = \"hyphentoken\"\n",
        "        self.numericalcommatoken = \"numericalcommatoken\"\n",
        "\n",
        "        self._initialize_special_tokens()\n",
        "        self._load_and_preprocess()\n",
        "\n",
        "    def _initialize_special_tokens(self):\n",
        "        self.word2index[self.START_TOKEN] = 0\n",
        "        self.word2index[self.END_TOKEN] = 1\n",
        "        self.word2index[self.UNK_TOKEN] = 2\n",
        "        self.word2index[self.hyphentoken] = 3\n",
        "        self.word2index[self.numericalcommatoken] = 4\n",
        "\n",
        "        self.index2word[0] = self.START_TOKEN\n",
        "        self.index2word[1] = self.END_TOKEN\n",
        "        self.index2word[2] = self.UNK_TOKEN\n",
        "        self.index2word[3] = self.hyphentoken\n",
        "        self.index2word[4] = self.numericalcommatoken\n",
        "\n",
        "\n",
        "    def clean_and_tokenize(self, corpus):\n",
        "        corpus = corpus.lower()  # Convert to lowercase\n",
        "        sentences = sent_tokenize(corpus)\n",
        "        cleaned_sentences = []\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()  # Remove unnecessary whitespaces\n",
        "            if not (sentence.startswith('=') or sentence.endswith('=')):  # Exclude headers\n",
        "                sentence = sentence.replace('<unk>', 'unknowntoken')\n",
        "                sentence = sentence.replace('@-@', 'hyphentoken')\n",
        "                sentence = sentence.replace('@,@', 'numericalcommatoken')\n",
        "                cleaned_sentences.append(sentence)\n",
        "        return cleaned_sentences\n",
        "\n",
        "    def _load_and_preprocess(self):\n",
        "        with open(self.corpus_path, 'r') as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Split the text into sentences using NLTK\n",
        "        self.cleaned_sentences = self.clean_and_tokenize(corpus)\n",
        "        # Count word frequencies\n",
        "        for sentence in self.cleaned_sentences:\n",
        "            words = word_tokenize(sentence)\n",
        "            for word in words:\n",
        "                word = word.lower()\n",
        "                self.word_freqs[word] += 1\n",
        "\n",
        "        # Build the vocabulary using only words that meet the frequency threshold\n",
        "        for word, freq in self.word_freqs.items():\n",
        "            if freq >= self.min_freq:\n",
        "                index = self.word2index[word]\n",
        "                self.index2word[index] = word\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.word2index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKeWEDHoJU8w"
      },
      "source": [
        "The code cell below introduces `WikiText2Dataset`, a subclass of PyTorch's `Dataset`. It's initialized with a preprocessor containing the text corpus and a specified context size for training. The class processes the text data into input-output pairs for language modeling, where the input is a sequence of words and the output is the word following the sequence. Special tokens and unknown words are managed according to predefined rules. The `__len__` and `__getitem__` methods allow the class to interface with PyTorch's DataLoader for batched data retrieval. Additionally, a `sample` method is included for displaying examples from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3thP8lFJU8w"
      },
      "outputs": [],
      "source": [
        "class WikiText2Dataset(Dataset):\n",
        "    def __init__(self, preprocessor, context_size):\n",
        "        super(WikiText2Dataset, self).__init__()\n",
        "\n",
        "        self.context_size = context_size\n",
        "\n",
        "        # We already have cleaned sentences in the preprocessor\n",
        "        self.sentences = preprocessor.cleaned_sentences\n",
        "        self.word2index = preprocessor.word2index\n",
        "        self.index2word = preprocessor.index2word\n",
        "        self.word_freqs = preprocessor.word_freqs\n",
        "        self.START_TOKEN = preprocessor.START_TOKEN\n",
        "        self.END_TOKEN = preprocessor.END_TOKEN\n",
        "        self.UNK_TOKEN = preprocessor.UNK_TOKEN\n",
        "        self.hyphentoken = preprocessor.hyphentoken\n",
        "        self.numericalcommatoken = preprocessor.numericalcommatoken\n",
        "\n",
        "\n",
        "        self.X, self.Y = self._build_dataset()\n",
        "\n",
        "    def _build_dataset(self):\n",
        "        X, Y = [], []\n",
        "\n",
        "        for sentence in self.sentences:\n",
        "            words = word_tokenize(sentence)\n",
        "            if not words:\n",
        "                continue\n",
        "            if words[-1] in string.punctuation:\n",
        "                words[-1] = self.END_TOKEN\n",
        "            else:\n",
        "                words.append(self.END_TOKEN)\n",
        "\n",
        "            context = [0] * self.context_size\n",
        "            for i, word in enumerate(words):\n",
        "\n",
        "                if word in self.word2index and word not in ['unknowntoken', 'hyphentoken', 'numericalcommatoken']:\n",
        "                    index = self.word2index[word]\n",
        "                elif word == 'unknowntoken':\n",
        "                    index = self.word2index[self.UNK_TOKEN]\n",
        "                elif word == 'hyphentoken':\n",
        "                    index = self.word2index[self.hyphentoken]\n",
        "                elif word == 'numericalcommatoken':\n",
        "                   index = self.word2index[self.numericalcommatoken]\n",
        "                else:\n",
        "                    index = self.word2index[self.UNK_TOKEN]\n",
        "                X.append(context)\n",
        "                Y.append(index)\n",
        "                context = context[1:] + [index]\n",
        "\n",
        "        return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.Y[index]\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        for _ in range(num_samples):\n",
        "            idx = torch.randint(0, len(self), (1,)).item()\n",
        "            context, target = self.X[idx], self.Y[idx]\n",
        "            context_words = [self.index2word[i.item()] for i in context]\n",
        "            target_word = self.index2word[target.item()]\n",
        "            print(\" \".join(context_words), \"------>\", target_word)\n",
        "\n",
        "    def get_context_size(self):\n",
        "        return self.context_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egboFjtZJU8x"
      },
      "source": [
        "The code cell below instantiates `train_vocabBuilder` with a specified corpus path and minimum word frequency, constructing a vocabulary. It then initializes `train_dataset` and `valid_dataset` using the `WikiText2Dataset` class, both employing the vocabulary and context size from `train_vocabBuilder`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHo8cmoLJU8x"
      },
      "outputs": [],
      "source": [
        "train_vocabBuilder =  WikiText2VocabBuilder(corpus_path = training_data_corpus_path, min_freq=min_freq)\n",
        "train_dataset = WikiText2Dataset(train_vocabBuilder, context_size = context_size)\n",
        "valid_dataset = WikiText2Dataset(train_vocabBuilder, context_size = context_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sznz-sBJU8y"
      },
      "source": [
        "The code cell below sets up data loaders for both the training and validation datasets using PyTorch's `DataLoader` class, with specified batch size, shuffle setting, and number of worker processes. It also logs and prints the context size, the vocabulary size from the `train_vocabBuilder`, and the number of context-target pairs in both the training and validation datasets. Lastly, it fetches and logs a sample of 10 context-target pairs from the training dataset to provide a glimpse into the prepared data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xjD0QJEJU8y",
        "outputId": "e600598b-266d-49a9-f22a-391691afa6da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context size: 10\n",
            "Number of tokens in the vocabulary: 28685\n",
            "Number of context-target pairs in the training corpus: 1857512\n",
            "Number of context-target pairs in the validation corpus: 1857512\n",
            ", and the last exit 's new owners moved it ------> to\n",
            "anthropology in 1935 , he developed a lifelong interest in ------> the\n",
            "<s> <s> in july 2006 , bosi and his wife ------> claire\n",
            "publisher of the french magazine le moniteur de la mode ------> </s>\n",
            "accused of copying the tune of the song `` <unk> ------> ``\n",
            "setting in motion of the wheel of dharma `` , ------> at\n",
            ", but the focus seems to be on his groin ------> rather\n",
            "costa rica , french <unk> , guatemala , honduras , ------> nicaragua\n",
            "by other names such as <unk> <unk> ( `` harvest ------> of\n",
            "official standards for religious instruction until the fourth lateran council ------> in\n",
            "Sample of the training corpus: None\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "# Log and print other details\n",
        "context_size_msg = f\"Context size: {context_size}\"\n",
        "print(context_size_msg)\n",
        "logging.info(context_size_msg)\n",
        "\n",
        "vocab_size_msg = f\"Number of tokens in the vocabulary: {train_vocabBuilder.vocab_size()}\"\n",
        "print(vocab_size_msg)\n",
        "logging.info(vocab_size_msg)\n",
        "\n",
        "train_dataset_size_msg = f\"Number of context-target pairs in the training corpus: {len(train_dataset)}\"\n",
        "print(train_dataset_size_msg)\n",
        "logging.info(train_dataset_size_msg)\n",
        "\n",
        "valid_dataset_size_msg = f\"Number of context-target pairs in the validation corpus: {len(valid_dataset)}\"\n",
        "print(valid_dataset_size_msg)\n",
        "logging.info(valid_dataset_size_msg)\n",
        "\n",
        "\n",
        "\n",
        "sample_train_data = str(train_dataset.sample(10))\n",
        "print(f\"Sample of the training corpus: {sample_train_data}\")\n",
        "logging.info(f\"Sample of the training corpus: {sample_train_data}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG5RzvBVJU8z"
      },
      "source": [
        "The code cell below defines a `LanguageModel` class, extending PyTorch's `nn.Module`. This model includes an embedding layer, several hidden layers with ReLU activation and layer normalization, and an output layer. It is initialized with the size of the vocabulary, embedding dimension, hidden layer size, context size, and number of hidden layers. The network uses residual connections between layers and Kaiming (He) initialization for weights to aid in training deep networks. The `forward` method implements the forward pass, which computes the log probabilities of the next word given a context. It also includes a method `num_parameters` to return the count of trainable parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toErjbqOJU8z"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, context_size, numberOfLayers):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.context_size = context_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.numberOfLayers = numberOfLayers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        self.ReLU = nn.ReLU()  # Defining a single instance of ReLU to reuse\n",
        "\n",
        "\n",
        "        self.embeddingToHiddenLayer = nn.Linear((self.context_size) * self.embed_size, self.hidden_size)\n",
        "        self.hiddenToOutputLayer = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "\n",
        "        # Hidden layers weights, bias, and layer normalization\n",
        "        for i in range(self.numberOfLayers):\n",
        "            linear_layer = nn.Linear((self.context_size) * self.embed_size if i == 0 else self.hidden_size, self.hidden_size)\n",
        "            layer_norm = nn.LayerNorm(self.hidden_size)\n",
        "            ReLULayer = nn.ReLU()\n",
        "\n",
        "\n",
        "            self.hidden_layers.append(nn.Sequential(\n",
        "                linear_layer,\n",
        "                layer_norm,\n",
        "                self.ReLU\n",
        "            ))\n",
        "\n",
        "        self.output_layer = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Xavier initialization for embedding\n",
        "            nn.init.kaiming_normal_(self.embedding.weight)\n",
        "            nn.init.kaiming_normal_(self.embeddingToHiddenLayer.weight)\n",
        "            nn.init.constant_(self.embeddingToHiddenLayer.bias, 0)\n",
        "\n",
        "            nn.init.kaiming_normal_(self.hiddenToOutputLayer.weight)\n",
        "            nn.init.constant_(self.hiddenToOutputLayer.bias, 0)\n",
        "\n",
        "\n",
        "            # Kaiming initialization for linear layers\n",
        "            for hidden_layer in self.hidden_layers:\n",
        "                nn.init.kaiming_normal_(hidden_layer[0].weight)\n",
        "\n",
        "            # Initialize batch normalization layers\n",
        "            for hidden_layer in self.hidden_layers:\n",
        "                nn.init.constant_(hidden_layer[1].weight, 1)\n",
        "                nn.init.constant_(hidden_layer[1].bias, 0)\n",
        "\n",
        "            # Make the output layer less confident\n",
        "            nn.init.constant_(self.output_layer.weight, 0.01)\n",
        "            nn.init.constant_(self.output_layer.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # Retrieve the corresponding embeddings\n",
        "\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        residual = self.embeddingToHiddenLayer(x)\n",
        "\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x) + residual\n",
        "            residual = x\n",
        "\n",
        "        residual = self.hiddenToOutputLayer(x)\n",
        "        y = self.output_layer(x) + residual\n",
        "\n",
        "        # Log probabilities (logits). Log probs is of shape (batch_size, vocab_size)\n",
        "        log_probs = F.log_softmax(y, dim=1)\n",
        "\n",
        "        return log_probs\n",
        "\n",
        "    def num_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giBHkHaCJU80"
      },
      "source": [
        "The function `train_language_model` implements the training process of our neural language model on a given dataset. It involves several key steps:\n",
        "\n",
        "1. **Inner Evaluation Function**: It defines an `evaluate` function that calculates the model's performance on a dataloader (usually the validation set). This function computes the perplexity, a common metric in language models, which quantifies how well the model predicts a sample.\n",
        "\n",
        "2. **Loss Function and Optimizer**: It utilizes the negative log-likelihood loss (`NLLLoss`), suitable for classification tasks with log probabilities as outputs. The AdamW optimizer is employed with a specified initial learning rate and weight decay, which helps prevent overfitting.\n",
        "\n",
        "3. **Learning Rate Scheduler**: An exponential decay scheduler is applied to the learning rate, aiming to reduce it from `start_lr` to `end_lr` over the total epochs.\n",
        "\n",
        "4. **Training Loop**: The model iterates over the training data loader for the specified number of epochs. Within each epoch, the model performs the following steps:\n",
        "   - Performs a forward pass to compute log probabilities.\n",
        "   - Calculates the loss between predictions and targets.\n",
        "   - Executes a backward pass to compute gradients.\n",
        "   - Updates model parameters using the optimizer.\n",
        "\n",
        "5. **Logging and Early Stopping**: During training, it logs statistics such as loss and perplexity, and checks for model improvement on the validation set to decide on early stopping. If there's no improvement for a number of epochs equal to `patience`, training halts early.\n",
        "\n",
        "6. **Timeout for Epochs**: Each epoch has a time limit, and if exceeded, the training is stopped for the current set of hyperparameters.\n",
        "\n",
        "7. **Overflow Handling**: If there is a potential overflow in computing perplexity (exceeding the largest representable float), the function stops training to prevent numerical instability.\n",
        "\n",
        "The function returns the learning rate schedule, training losses, training perplexities, and validation perplexities for further analysis and insight into the training process. This comprehensive approach ensures that the model is not only trained but also monitored for performance and computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9AeFSrMJU80"
      },
      "outputs": [],
      "source": [
        "def train_language_model(model, train_dataloader, valid_dataloader, device, total_epochs, start_lr, end_lr, weight_decay_value, patience):\n",
        "\n",
        "    def evaluate(model, dataloader, loss_function, device):\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        total_tokens = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for contexts, targets in dataloader:\n",
        "               contexts, targets = contexts.to(device), targets.to(device)\n",
        "               log_probs = model(contexts)\n",
        "               loss = loss_function(log_probs, targets)\n",
        "               total_loss += loss.item() * targets.size(0)\n",
        "               total_tokens += targets.size(0)\n",
        "\n",
        "        model.train()\n",
        "        return math.exp(total_loss / total_tokens)\n",
        "\n",
        "    loss_function = nn.NLLLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, weight_decay=weight_decay_value)\n",
        "    scheduler = ExponentialLR(optimizer, gamma=(end_lr/start_lr)**(1/total_epochs))\n",
        "\n",
        "    # Lists to store learning rate, loss, and perplexity values\n",
        "    learning_rates = []\n",
        "    losses = []\n",
        "    train_perplexities = []\n",
        "    valid_perplexities = []\n",
        "\n",
        "    best_valid_perplexity = float('inf')\n",
        "    no_improvement_counter = 0\n",
        "\n",
        "    epoch_time_limit = 3600  # 60 minutes in seconds, adjust as needed\n",
        "\n",
        "    for epoch in range(total_epochs):\n",
        "        epoch_start_time = time.time()  # Record the start time of the epoch\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for i, (contexts, targets) in enumerate(train_dataloader):\n",
        "            # Update learning rate according to the custom schedule\n",
        "\n",
        "            # Move the data to the GPU\n",
        "            contexts, targets = contexts.to(device), targets.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            log_probs = model(contexts)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_function(log_probs, targets)\n",
        "\n",
        "            # Accumulate total loss and total tokens\n",
        "            total_loss += loss.item() * targets.size(0)\n",
        "            total_tokens += targets.size(0)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Check for potential overflow in exponentiation\n",
        "        exponent = total_loss / total_tokens\n",
        "        if exponent > math.log(sys.float_info.max):\n",
        "            overflow_msg = f\"Potential overflow detected at epoch {epoch + 1}. Stopping training for this hyperparameter set.\"\n",
        "            logging.info(overflow_msg)\n",
        "            return None, None, [np.inf], [np.inf], np.inf  # Return inf for all metrics\n",
        "\n",
        "        train_perplexity = math.exp(exponent)\n",
        "\n",
        "        # Store values\n",
        "        losses.append(total_loss / total_tokens)\n",
        "        train_perplexities.append(train_perplexity)\n",
        "\n",
        "        learning_rate = optimizer.param_groups[0]['lr']\n",
        "        learning_rates.append(learning_rate)\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        valid_perplexity = evaluate(model, valid_dataloader, loss_function, device)\n",
        "\n",
        "        # Check if the validation perplexity has improved\n",
        "        if valid_perplexity < best_valid_perplexity:\n",
        "            best_valid_perplexity = valid_perplexity\n",
        "            no_improvement_counter = 0\n",
        "        else:\n",
        "            no_improvement_counter += 1\n",
        "        if no_improvement_counter >= patience:\n",
        "            early_stopping_msg = f\"Early stopping triggered after {epoch + 1} epochs.\"\n",
        "            logging.info(early_stopping_msg)\n",
        "            print(early_stopping_msg)\n",
        "            break\n",
        "\n",
        "        valid_perplexities.append(valid_perplexity)\n",
        "\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step()\n",
        "        stat_message = f\"Epoch {epoch+1}/{total_epochs} - Training Loss: {total_loss / total_tokens:.4f} - Training Perplexity: {train_perplexities[-1]:.4f} - Validation Perplexity: {valid_perplexities[-1]:.4f} - Learning rate: {learning_rate:.10f}\"\n",
        "        logging.info(stat_message)\n",
        "        print(stat_message)\n",
        "\n",
        "\n",
        "        # Reset total loss and total tokens for the next epoch\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "         # Check if the epoch has exceeded the time limit\n",
        "        if time.time() - epoch_start_time > epoch_time_limit:\n",
        "            timeout_msg = f\"Epoch {epoch+1} timed out after {epoch_time_limit/60:.2f} minutes. Stopping training for this hyperparameter set.\"\n",
        "            logging.info(timeout_msg)\n",
        "            print(timeout_msg)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    return learning_rates, losses, train_perplexities, valid_perplexities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwpd_fNLJU81"
      },
      "source": [
        "The `sample_logHyperparameters` function samples starting learning rate, end learning rate, and weight decay from log-uniform distributions defined by their respective ranges. It ensures that the hyperparameters are distributed evenly across orders of magnitude, returning the computed values for use in optimizing a machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8j8DwWmJU81"
      },
      "outputs": [],
      "source": [
        "def sample_logHyperparameters(lr_start_range, weight_decay_range, reduction_factor_range):\n",
        "    log_lr_start_range = np.log10(lr_start_range)\n",
        "    log_weight_decay_range = np.log10(weight_decay_range)\n",
        "    start_lr_log = np.random.uniform(log_lr_start_range[0], log_lr_start_range[1])\n",
        "    start_lr = 10**start_lr_log\n",
        "    log_reduction_factor_range = np.log10(reduction_factor_range)\n",
        "    log_reduction_factor = np.random.uniform(log_reduction_factor_range[0], log_reduction_factor_range[1])\n",
        "    reduction_factor = 10**log_reduction_factor\n",
        "    end_lr = start_lr * reduction_factor\n",
        "    weight_decay_log = np.random.uniform(log_weight_decay_range[0], log_weight_decay_range[1])\n",
        "    weight_decay = 10**weight_decay_log\n",
        "    return start_lr, end_lr, weight_decay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2CtaF4PJU81"
      },
      "source": [
        "The function `hyper_param_tuning` serves as a wrapper for `train_language_model`, passing through parameters for a neural network model's architecture and training process. It triggers the training of the model with specified layers, vocabulary size, embedding size, hidden size, context size, data loaders, device, epochs, learning rates, weight decay, and patience for early stopping. The function returns the outputs of the training process, typically learning rates, losses, and perplexities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25maWCmQJU81"
      },
      "outputs": [],
      "source": [
        "def hyper_param_tuning(model, numberOfLayers, vocab_size, embed_size, hidden_size, context_size, train_dataloader, valid_dataloader, device, total_epochs, start_lr, end_lr, weight_decay, patience):\n",
        "    return train_language_model(model=model, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader, device=device, total_epochs=total_epochs, start_lr=start_lr, end_lr=end_lr, weight_decay_value=weight_decay, patience=patience)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFHayuQ7JU82"
      },
      "source": [
        "The `sample_uniformHyperparameters` function randomly samples values for the number of layers, embedding size, and hidden size from specified uniform distributions. It uses the provided ranges for each hyperparameter to ensure the sampled values fall within the expected bounds. The function returns these sampled values for constructing a neural network model's architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7PCe63iJU82"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sample_uniformHyperparameters(numberOfLayers_range, embed_size_range, hidden_size_range):\n",
        "    # Randomly sample the number of layers, embedding size, and hidden size from their respective ranges\n",
        "    numberOfLayers = np.random.randint(numberOfLayers_range[0], numberOfLayers_range[1] + 1)\n",
        "    embedSize = np.random.randint(embed_size_range[0], embed_size_range[1] + 1)\n",
        "    hiddenSize = np.random.randint(hidden_size_range[0], hidden_size_range[1] + 1)\n",
        "\n",
        "    return numberOfLayers, embedSize, hiddenSize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_3Wy5PcJU82"
      },
      "source": [
        "The code cell sets up and executes a hyperparameter tuning loop for a language model, storing the results in `results_dict`. Each iteration samples hyperparameters, constructs a model, and trains it. The results, including the number of layers, embedding size, hidden size, learning rates, weight decay, training and validation perplexities, and the number of parameters, are recorded in `iter_dict`. This dictionary is then added to `results_dict` with a key indicating the iteration number. The best validation perplexity is tracked, and the detailed results are saved to a JSON file after each iteration. Finally, the best validation perplexity achieved across all iterations is printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x-aSEwVJU82",
        "outputId": "3a5d3373-1beb-44f9-8498-d47271003210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Device: cuda\n",
            "Hyperparameter Combination 1\n",
            "Number of Model Parameters: 25413365\n",
            "Sampled Hyperparameters: Start LR: 0.0003682380, End LR: 0.0000006549, Weight Decay: 0.0000241020, number of Layers: 7, Embedding Size: 167, Hidden Size: 328\n",
            "Epoch 1/15 - Training Loss: 6.1666 - Training Perplexity: 476.5729 - Validation Perplexity: 190.0661 - Learning rate: 0.0003682380\n",
            "Epoch 2/15 - Training Loss: 5.1291 - Training Perplexity: 168.8713 - Validation Perplexity: 101.6181 - Learning rate: 0.0002414342\n",
            "Epoch 3/15 - Training Loss: 4.6079 - Training Perplexity: 100.2715 - Validation Perplexity: 66.2651 - Learning rate: 0.0001582956\n",
            "Epoch 4/15 - Training Loss: 4.2343 - Training Perplexity: 69.0122 - Validation Perplexity: 50.4053 - Learning rate: 0.0001037861\n",
            "Epoch 5/15 - Training Loss: 3.9760 - Training Perplexity: 53.3009 - Validation Perplexity: 42.2767 - Learning rate: 0.0000680471\n",
            "Epoch 6/15 - Training Loss: 3.8001 - Training Perplexity: 44.7054 - Validation Perplexity: 37.9589 - Learning rate: 0.0000446149\n",
            "Epoch 7/15 - Training Loss: 3.6816 - Training Perplexity: 39.7108 - Validation Perplexity: 35.4584 - Learning rate: 0.0000292516\n",
            "Epoch 8/15 - Training Loss: 3.6024 - Training Perplexity: 36.6850 - Validation Perplexity: 33.9161 - Learning rate: 0.0000191787\n",
            "Epoch 9/15 - Training Loss: 3.5495 - Training Perplexity: 34.7973 - Validation Perplexity: 32.9918 - Learning rate: 0.0000125745\n",
            "Epoch 10/15 - Training Loss: 3.5144 - Training Perplexity: 33.5963 - Validation Perplexity: 32.4162 - Learning rate: 0.0000082444\n",
            "Epoch 11/15 - Training Loss: 3.4912 - Training Perplexity: 32.8251 - Validation Perplexity: 32.0444 - Learning rate: 0.0000054054\n",
            "Epoch 12/15 - Training Loss: 3.4758 - Training Perplexity: 32.3247 - Validation Perplexity: 31.8077 - Learning rate: 0.0000035441\n",
            "Epoch 13/15 - Training Loss: 3.4656 - Training Perplexity: 31.9971 - Validation Perplexity: 31.6555 - Learning rate: 0.0000023237\n",
            "Epoch 14/15 - Training Loss: 3.4590 - Training Perplexity: 31.7836 - Validation Perplexity: 31.5578 - Learning rate: 0.0000015235\n",
            "Epoch 15/15 - Training Loss: 3.4545 - Training Perplexity: 31.6431 - Validation Perplexity: 31.4942 - Learning rate: 0.0000009989\n",
            "Start LR: 0.0003682380, End LR: 0.0000006549, Weight Decay: 0.0000241020, Validation Perplexity: 31.4942, Training Loss: 3.4545, Training Perplexity: 31.6431\n",
            "Hyperparameter Combination 2\n",
            "Number of Model Parameters: 81294423\n",
            "Sampled Hyperparameters: Start LR: 0.0006550352, End LR: 0.0000007037, Weight Decay: 0.0005892217, number of Layers: 5, Embedding Size: 357, Hidden Size: 1034\n",
            "Epoch 1/15 - Training Loss: 5.8906 - Training Perplexity: 361.6103 - Validation Perplexity: 81.9832 - Learning rate: 0.0006550352\n",
            "Epoch 2/15 - Training Loss: 4.2445 - Training Perplexity: 69.7217 - Validation Perplexity: 16.3537 - Learning rate: 0.0004152784\n",
            "Epoch 3/15 - Training Loss: 2.7794 - Training Perplexity: 16.1089 - Validation Perplexity: 6.4097 - Learning rate: 0.0002632776\n",
            "Epoch 4/15 - Training Loss: 1.9274 - Training Perplexity: 6.8717 - Validation Perplexity: 4.0547 - Learning rate: 0.0001669124\n",
            "Epoch 5/15 - Training Loss: 1.4611 - Training Perplexity: 4.3105 - Validation Perplexity: 3.1121 - Learning rate: 0.0001058189\n",
            "Epoch 6/15 - Training Loss: 1.1827 - Training Perplexity: 3.2631 - Validation Perplexity: 2.6486 - Learning rate: 0.0000670869\n",
            "Epoch 7/15 - Training Loss: 1.0120 - Training Perplexity: 2.7510 - Validation Perplexity: 2.4057 - Learning rate: 0.0000425317\n",
            "Epoch 8/15 - Training Loss: 0.9066 - Training Perplexity: 2.4760 - Validation Perplexity: 2.2722 - Learning rate: 0.0000269642\n",
            "Epoch 9/15 - Training Loss: 0.8414 - Training Perplexity: 2.3196 - Validation Perplexity: 2.1949 - Learning rate: 0.0000170947\n",
            "Epoch 10/15 - Training Loss: 0.8004 - Training Perplexity: 2.2265 - Validation Perplexity: 2.1481 - Learning rate: 0.0000108377\n",
            "Epoch 11/15 - Training Loss: 0.7747 - Training Perplexity: 2.1699 - Validation Perplexity: 2.1202 - Learning rate: 0.0000068709\n",
            "Epoch 12/15 - Training Loss: 0.7583 - Training Perplexity: 2.1347 - Validation Perplexity: 2.1033 - Learning rate: 0.0000043560\n",
            "Epoch 13/15 - Training Loss: 0.7480 - Training Perplexity: 2.1128 - Validation Perplexity: 2.0927 - Learning rate: 0.0000027616\n",
            "Epoch 14/15 - Training Loss: 0.7414 - Training Perplexity: 2.0989 - Validation Perplexity: 2.0861 - Learning rate: 0.0000017508\n",
            "Epoch 15/15 - Training Loss: 0.7373 - Training Perplexity: 2.0902 - Validation Perplexity: 2.0821 - Learning rate: 0.0000011100\n",
            "Start LR: 0.0006550352, End LR: 0.0000007037, Weight Decay: 0.0005892217, Validation Perplexity: 2.0821, Training Loss: 0.7373, Training Perplexity: 2.0902\n",
            "Hyperparameter Combination 3\n",
            "Number of Model Parameters: 60674778\n",
            "Sampled Hyperparameters: Start LR: 0.0000106331, End LR: 0.0000000128, Weight Decay: 0.0000124432, number of Layers: 8, Embedding Size: 324, Hidden Size: 743\n",
            "Epoch 1/15 - Training Loss: 7.3785 - Training Perplexity: 1601.1363 - Validation Perplexity: 809.9726 - Learning rate: 0.0000106331\n",
            "Epoch 2/15 - Training Loss: 6.6042 - Training Perplexity: 738.2066 - Validation Perplexity: 639.3987 - Learning rate: 0.0000067909\n",
            "Epoch 3/15 - Training Loss: 6.4226 - Training Perplexity: 615.6242 - Validation Perplexity: 566.0599 - Learning rate: 0.0000043370\n",
            "Epoch 4/15 - Training Loss: 6.3210 - Training Perplexity: 556.1427 - Validation Perplexity: 527.0326 - Learning rate: 0.0000027699\n",
            "Epoch 5/15 - Training Loss: 6.2596 - Training Perplexity: 522.9925 - Validation Perplexity: 505.1205 - Learning rate: 0.0000017690\n",
            "Epoch 6/15 - Training Loss: 6.2212 - Training Perplexity: 503.2850 - Validation Perplexity: 491.8784 - Learning rate: 0.0000011298\n",
            "Epoch 7/15 - Training Loss: 6.1969 - Training Perplexity: 491.2178 - Validation Perplexity: 483.8591 - Learning rate: 0.0000007215\n",
            "Epoch 8/15 - Training Loss: 6.1814 - Training Perplexity: 483.6740 - Validation Perplexity: 478.9306 - Learning rate: 0.0000004608\n",
            "Epoch 9/15 - Training Loss: 6.1715 - Training Perplexity: 478.9011 - Validation Perplexity: 475.8297 - Learning rate: 0.0000002943\n",
            "Epoch 10/15 - Training Loss: 6.1652 - Training Perplexity: 475.8748 - Validation Perplexity: 473.8847 - Learning rate: 0.0000001880\n",
            "Epoch 11/15 - Training Loss: 6.1611 - Training Perplexity: 473.9421 - Validation Perplexity: 472.6582 - Learning rate: 0.0000001200\n",
            "Epoch 12/15 - Training Loss: 6.1584 - Training Perplexity: 472.6920 - Validation Perplexity: 471.8564 - Learning rate: 0.0000000767\n",
            "Epoch 13/15 - Training Loss: 6.1567 - Training Perplexity: 471.8794 - Validation Perplexity: 471.3376 - Learning rate: 0.0000000490\n",
            "Epoch 14/15 - Training Loss: 6.1556 - Training Perplexity: 471.3506 - Validation Perplexity: 471.0033 - Learning rate: 0.0000000313\n",
            "Epoch 15/15 - Training Loss: 6.1549 - Training Perplexity: 471.0132 - Validation Perplexity: 470.7967 - Learning rate: 0.0000000200\n",
            "Start LR: 0.0000106331, End LR: 0.0000000128, Weight Decay: 0.0000124432, Validation Perplexity: 470.7967, Training Loss: 6.1549, Training Perplexity: 471.0132\n",
            "Hyperparameter Combination 4\n",
            "Number of Model Parameters: 42401630\n",
            "Sampled Hyperparameters: Start LR: 0.0001385759, End LR: 0.0000044242, Weight Decay: 0.0000300718, number of Layers: 4, Embedding Size: 318, Hidden Size: 509\n",
            "Epoch 1/15 - Training Loss: 6.2617 - Training Perplexity: 524.0993 - Validation Perplexity: 224.2886 - Learning rate: 0.0001385759\n",
            "Epoch 2/15 - Training Loss: 5.2689 - Training Perplexity: 194.2071 - Validation Perplexity: 113.6802 - Learning rate: 0.0001101449\n",
            "Epoch 3/15 - Training Loss: 4.6978 - Training Perplexity: 109.7045 - Validation Perplexity: 66.8669 - Learning rate: 0.0000875469\n",
            "Epoch 4/15 - Training Loss: 4.2455 - Training Perplexity: 69.7879 - Validation Perplexity: 44.9082 - Learning rate: 0.0000695853\n",
            "Epoch 5/15 - Training Loss: 3.8844 - Training Perplexity: 48.6394 - Validation Perplexity: 33.2234 - Learning rate: 0.0000553088\n",
            "Epoch 6/15 - Training Loss: 3.5944 - Training Perplexity: 36.3933 - Validation Perplexity: 26.3572 - Learning rate: 0.0000439613\n",
            "Epoch 7/15 - Training Loss: 3.3635 - Training Perplexity: 28.8908 - Validation Perplexity: 22.1317 - Learning rate: 0.0000349420\n",
            "Epoch 8/15 - Training Loss: 3.1805 - Training Perplexity: 24.0596 - Validation Perplexity: 19.3246 - Learning rate: 0.0000277731\n",
            "Epoch 9/15 - Training Loss: 3.0360 - Training Perplexity: 20.8220 - Validation Perplexity: 17.4156 - Learning rate: 0.0000220750\n",
            "Epoch 10/15 - Training Loss: 2.9218 - Training Perplexity: 18.5753 - Validation Perplexity: 16.0481 - Learning rate: 0.0000175460\n",
            "Epoch 11/15 - Training Loss: 2.8311 - Training Perplexity: 16.9649 - Validation Perplexity: 15.0808 - Learning rate: 0.0000139461\n",
            "Epoch 12/15 - Training Loss: 2.7594 - Training Perplexity: 15.7911 - Validation Perplexity: 14.3442 - Learning rate: 0.0000110849\n",
            "Epoch 13/15 - Training Loss: 2.7026 - Training Perplexity: 14.9187 - Validation Perplexity: 13.8167 - Learning rate: 0.0000088106\n",
            "Epoch 14/15 - Training Loss: 2.6574 - Training Perplexity: 14.2598 - Validation Perplexity: 13.3989 - Learning rate: 0.0000070030\n",
            "Epoch 15/15 - Training Loss: 2.6216 - Training Perplexity: 13.7577 - Validation Perplexity: 13.0847 - Learning rate: 0.0000055662\n",
            "Start LR: 0.0001385759, End LR: 0.0000044242, Weight Decay: 0.0000300718, Validation Perplexity: 13.0847, Training Loss: 2.6216, Training Perplexity: 13.7577\n",
            "Hyperparameter Combination 5\n",
            "Number of Model Parameters: 65086525\n",
            "Sampled Hyperparameters: Start LR: 0.0005845100, End LR: 0.0000009362, Weight Decay: 0.0000827672, number of Layers: 3, Embedding Size: 103, Hidden Size: 1010\n",
            "Epoch 1/15 - Training Loss: 5.9587 - Training Perplexity: 387.1033 - Validation Perplexity: 112.0747 - Learning rate: 0.0005845100\n",
            "Epoch 2/15 - Training Loss: 4.5724 - Training Perplexity: 96.7783 - Validation Perplexity: 32.5801 - Learning rate: 0.0003805659\n",
            "Epoch 3/15 - Training Loss: 3.5362 - Training Perplexity: 34.3379 - Validation Perplexity: 16.5090 - Learning rate: 0.0002477809\n",
            "Epoch 4/15 - Training Loss: 2.9308 - Training Perplexity: 18.7427 - Validation Perplexity: 11.7139 - Learning rate: 0.0001613265\n",
            "Epoch 5/15 - Training Loss: 2.5715 - Training Perplexity: 13.0856 - Validation Perplexity: 9.5034 - Learning rate: 0.0001050373\n",
            "Epoch 6/15 - Training Loss: 2.3414 - Training Perplexity: 10.3953 - Validation Perplexity: 8.3422 - Learning rate: 0.0000683882\n",
            "Epoch 7/15 - Training Loss: 2.1913 - Training Perplexity: 8.9469 - Validation Perplexity: 7.7033 - Learning rate: 0.0000445266\n",
            "Epoch 8/15 - Training Loss: 2.0930 - Training Perplexity: 8.1090 - Validation Perplexity: 7.3271 - Learning rate: 0.0000289906\n",
            "Epoch 9/15 - Training Loss: 2.0285 - Training Perplexity: 7.6029 - Validation Perplexity: 7.1022 - Learning rate: 0.0000188754\n",
            "Epoch 10/15 - Training Loss: 1.9863 - Training Perplexity: 7.2883 - Validation Perplexity: 6.9629 - Learning rate: 0.0000122895\n",
            "Epoch 11/15 - Training Loss: 1.9585 - Training Perplexity: 7.0885 - Validation Perplexity: 6.8764 - Learning rate: 0.0000080015\n",
            "Epoch 12/15 - Training Loss: 1.9402 - Training Perplexity: 6.9604 - Validation Perplexity: 6.8215 - Learning rate: 0.0000052097\n",
            "Epoch 13/15 - Training Loss: 1.9283 - Training Perplexity: 6.8778 - Validation Perplexity: 6.7870 - Learning rate: 0.0000033919\n",
            "Epoch 14/15 - Training Loss: 1.9205 - Training Perplexity: 6.8242 - Validation Perplexity: 6.7648 - Learning rate: 0.0000022084\n",
            "Epoch 15/15 - Training Loss: 1.9154 - Training Perplexity: 6.7894 - Validation Perplexity: 6.7506 - Learning rate: 0.0000014379\n",
            "Start LR: 0.0005845100, End LR: 0.0000009362, Weight Decay: 0.0000827672, Validation Perplexity: 6.7506, Training Loss: 1.9154, Training Perplexity: 6.7894\n",
            "Hyperparameter Combination 6\n",
            "Number of Model Parameters: 76238908\n",
            "Sampled Hyperparameters: Start LR: 0.0007125891, End LR: 0.0000007368, Weight Decay: 0.0000282426, number of Layers: 3, Embedding Size: 188, Hidden Size: 1117\n",
            "Epoch 1/15 - Training Loss: 5.8744 - Training Perplexity: 355.8091 - Validation Perplexity: 86.2849 - Learning rate: 0.0007125891\n",
            "Epoch 2/15 - Training Loss: 4.3175 - Training Perplexity: 75.0010 - Validation Perplexity: 20.1918 - Learning rate: 0.0004506172\n",
            "Epoch 3/15 - Training Loss: 3.0541 - Training Perplexity: 21.2017 - Validation Perplexity: 9.3464 - Learning rate: 0.0002849550\n",
            "Epoch 4/15 - Training Loss: 2.3344 - Training Perplexity: 10.3229 - Validation Perplexity: 6.2608 - Learning rate: 0.0001801959\n",
            "Epoch 5/15 - Training Loss: 1.9233 - Training Perplexity: 6.8434 - Validation Perplexity: 4.9479 - Learning rate: 0.0001139498\n",
            "Epoch 6/15 - Training Loss: 1.6680 - Training Perplexity: 5.3018 - Validation Perplexity: 4.2758 - Learning rate: 0.0000720580\n",
            "Epoch 7/15 - Training Loss: 1.5063 - Training Perplexity: 4.5098 - Validation Perplexity: 3.9124 - Learning rate: 0.0000455670\n",
            "Epoch 8/15 - Training Loss: 1.4036 - Training Perplexity: 4.0697 - Validation Perplexity: 3.7059 - Learning rate: 0.0000288150\n",
            "Epoch 9/15 - Training Loss: 1.3382 - Training Perplexity: 3.8121 - Validation Perplexity: 3.5866 - Learning rate: 0.0000182216\n",
            "Epoch 10/15 - Training Loss: 1.2966 - Training Perplexity: 3.6570 - Validation Perplexity: 3.5148 - Learning rate: 0.0000115228\n",
            "Epoch 11/15 - Training Loss: 1.2701 - Training Perplexity: 3.5614 - Validation Perplexity: 3.4718 - Learning rate: 0.0000072866\n",
            "Epoch 12/15 - Training Loss: 1.2533 - Training Perplexity: 3.5019 - Validation Perplexity: 3.4450 - Learning rate: 0.0000046078\n",
            "Epoch 13/15 - Training Loss: 1.2426 - Training Perplexity: 3.4645 - Validation Perplexity: 3.4284 - Learning rate: 0.0000029138\n"
          ]
        }
      ],
      "source": [
        "vocab_size = train_vocabBuilder.vocab_size() # The size of the vocabulary\n",
        "\n",
        "# Initialize device based on the configuration and availability\n",
        "device = torch.device(config_device if torch.cuda.is_available() else \"cpu\")\n",
        "device_msg = f\"Used Device: {device}\"\n",
        "logging.info(device_msg)\n",
        "print(device_msg)\n",
        "valid_perplexities_dict = {}\n",
        "\n",
        "np.random.seed(423455335)\n",
        "# Set the seed for generating random numbers\n",
        "torch.manual_seed(423455335)\n",
        "# Set the seed for generating random numbers for CUDA when using the GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Initialize the dictionary that will hold all the data\n",
        "results_dict = {}\n",
        "\n",
        "# Save the best model's state\n",
        "best_valid_perplexity = float('inf')\n",
        "\n",
        "json_filename = \"hyperparameter_tuning_results.json\"\n",
        "\n",
        "for i in range(num_iterations):\n",
        "    start_lr, end_lr, weight_decay = sample_logHyperparameters(lr_start_range, weight_decay_range, reduction_factor_range)\n",
        "    numberOfLayers, embedSize, hiddenSize = sample_uniformHyperparameters(numberOfLayers_range, embed_size_range, hidden_size_range)\n",
        "    model = LanguageModel(vocab_size, embedSize, hiddenSize, context_size, numberOfLayers)\n",
        "    model = model.to(device)\n",
        "    hyper_comb_msg = f\"Hyperparameter Combination {i+1}\"\n",
        "    print(hyper_comb_msg)\n",
        "    logging.info(hyper_comb_msg)\n",
        "\n",
        "    model_param_msg = f\"Number of Model Parameters: {model.num_parameters()}\"\n",
        "    print(model_param_msg)\n",
        "    logging.info(model_param_msg)\n",
        "\n",
        "    sampled_hyper_msg = f\"Sampled Hyperparameters: Start LR: {start_lr:.10f}, End LR: {end_lr:.10f}, Weight Decay: {weight_decay:.10f}, number of Layers: {numberOfLayers}, Embedding Size: {embedSize}, Hidden Size: {hiddenSize}\"\n",
        "    print(sampled_hyper_msg)\n",
        "    logging.info(sampled_hyper_msg)\n",
        "    start_time = time.time()\n",
        "    learning_rates, losses, train_perplexities, valid_perplexities = hyper_param_tuning(model, numberOfLayers, vocab_size, embedSize, hiddenSize, context_size, train_dataloader, valid_dataloader, device, total_epochs, start_lr, end_lr, weight_decay, patience)\n",
        "\n",
        "    end_time = time.time()\n",
        "    time_per_iter = end_time - start_time\n",
        "    time_per_iter_msg = f\"Time taken for this iteration: {time_per_iter:.2f} seconds\"\n",
        "    if valid_perplexities[-1] == float('inf'):\n",
        "        inf_perplex_msg = \"Hyperparameter combination resulted in NaN perplexity. Skipping...\"\n",
        "        print(inf_perplex_msg)\n",
        "        logging.info(inf_perplex_msg)\n",
        "        continue\n",
        "\n",
        "    if valid_perplexities[-1] < best_valid_perplexity:\n",
        "        best_valid_perplexity = valid_perplexities[-1]\n",
        "\n",
        "    iter_dict = {\n",
        "        'numberOfLayers': numberOfLayers,\n",
        "        'embedSize': embedSize,\n",
        "        'hiddenSize': hiddenSize,\n",
        "        'start_lr': start_lr,\n",
        "        'end_lr': end_lr,\n",
        "        'weight_decay': weight_decay,\n",
        "        'time_per_iter': time_per_iter,\n",
        "        'numberOfParameters': model.num_parameters(),\n",
        "        'learning_rates': learning_rates,\n",
        "        'losses': losses,\n",
        "        'train_perplexities': train_perplexities,\n",
        "        'valid_perplexities': valid_perplexities,\n",
        "    }\n",
        "\n",
        "    # Add the current iteration's results to the main results dictionary\n",
        "    results_dict[f'iter_{i+1}'] = iter_dict\n",
        "    with open(json_filename, 'w') as f:\n",
        "        json.dump(results_dict, f, indent=4)\n",
        "    result_msg = f\"Start LR: {start_lr:.10f}, End LR: {end_lr:.10f}, Weight Decay: {weight_decay:.10f}, Validation Perplexity: {valid_perplexities[-1]:.4f}, Training Loss: {losses[-1]:.4f}, Training Perplexity: {train_perplexities[-1]:.4f}\"\n",
        "    print(result_msg)\n",
        "    logging.info(result_msg)\n",
        "\n",
        "    print(\" End of Hyperparameter Combination\")\n",
        "    print(\"=========================================\")\n",
        "\n",
        "\n",
        "print(f\"Best validation perplexity: {best_valid_perplexity:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOkx6qrfJU83"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "transformers",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}