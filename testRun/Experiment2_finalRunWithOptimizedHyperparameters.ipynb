{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this script we build, train, and evaluate a neural network model using the [WikiText-2 Dataset](https://paperswithcode.com/dataset/wikitext-2).\n",
        "\n",
        "The script begins by importing necessary libraries for data handling, neural network construction, and training. It then reads a configuration file, `testRunConfig.yaml`, which sets various parameters for data processing, model architecture, and training. These parameters include file paths for training, validation, and test datasets, minimum word frequency for vocabulary inclusion, context size for training, and various hyperparameters like learning rate, weight decay, and the architecture of the neural network. These parameters originate from experiment 3 where we implement a random hyperparameter search.\n",
        "\n",
        "A logging function is initialized to record the training process, creating a unique log file named with the current date and time. This aids in tracking and debugging the model's training process.\n",
        "\n",
        "\n",
        "The `WikiText2VocabBuilder` class is responsible for constructing the vocabulary from the text corpus. It preprocesses the text, tokenizes it, and builds a vocabulary, considering only words that meet a specified frequency threshold. This class also handles special tokens, ensuring consistent representation in the processed data.\n",
        "\n",
        "The `WikiText2Dataset` class, derived from PyTorch's `Dataset`, prepares the data for the language modeling task. It processes the text data into input-output pairs suitable for training a language model, where the input is a sequence of words, and the output is the word following this sequence.\n",
        "\n",
        "The `LanguageModel` class defines the neural network architecture for the language modeling task. It includes an embedding layer, multiple hidden layers with ReLU activation and layer normalization, and an output layer. The network is designed with residual connections and uses Kaiming initialization for its weights, which is beneficial for training deep networks.\n",
        "\n",
        "The `train_language_model` function encapsulates the training process. It uses negative log-likelihood loss and the AdamW optimizer, adjusting the learning rate over epochs using an exponential decay scheduler. The function evaluates the model's performance on the validation set using perplexity, a standard metric in language modeling. It implements early stopping based on validation performance and includes checks for potential numerical issues like overflow.\n",
        "\n",
        "After training, the model's weights are saved to a file for later use or further analysis. In summary, the script is a complete pipeline for training a neural network-based language model, with  data preprocessing, model architecture, training optimization, and results logging.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ypFFIx8Ww0Sg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soOi4YYbwyeO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import random\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "import string\n",
        "import math\n",
        "import sys\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "import json\n",
        "import string\n",
        "import math\n",
        "import sys\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "import json\n",
        "from torch.utils.tensorboard import SummaryWriter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the configuration."
      ],
      "metadata": {
        "id": "Wh2KfkZ19p7W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN_zt5TSwyeR"
      },
      "outputs": [],
      "source": [
        "with open(\"testRunConfig.yaml\", 'r') as ymlfile:\n",
        "    config = yaml.safe_load(ymlfile)\n",
        "\n",
        "training_data_corpus_path = config['data']['training_data_corpus_path']\n",
        "validation_data_corpus_path = config['data']['validation_data_corpus_path']\n",
        "test_data_corpus_path = config['data']['test_data_corpus_path']\n",
        "min_freq = config['data']['min_freq']\n",
        "context_size = config['data']['context_size']\n",
        "\n",
        "shuffle = config['runtime']['shuffle']\n",
        "num_workers = config['runtime']['num_workers']\n",
        "batch_size = config['runtime']['batch_size']\n",
        "config_device = config['runtime']['device']\n",
        "\n",
        "\n",
        "total_epochs = config['experiment']['total_epochs']\n",
        "patience = config['experiment']['patience']\n",
        "lr_start = config['hyperparameters']['lr_start']\n",
        "weight_decay = config['hyperparameters']['weight_decay']\n",
        "lr_end = config['hyperparameters']['lr_end']\n",
        "numberOfLayers = config['hyperparameters']['numberOfLayers'] # Number of hidden layers\n",
        "embed_size= config['hyperparameters']['embed_size']  # The embedding size of the words\n",
        "hidden_size = config['hyperparameters']['hidden_size'] # The hidden size of the neural network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing Logging"
      ],
      "metadata": {
        "id": "kp5MuP5i9s4d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmtFUIytwyeR",
        "outputId": "e12632b8-7d44-4e2d-f4ff-49bd7137c614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logging to file: testRun_2023-09-17_19-39-19.log\n"
          ]
        }
      ],
      "source": [
        "# Function to initialize logging with dynamic filename\n",
        "def init_logger():\n",
        "    # Generate a unique log file name based on the current date and time\n",
        "    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    log_file_name = f'testRun_{current_time}.log'\n",
        "\n",
        "    # Configure the logging module to write the logs to a file\n",
        "    logging.basicConfig(filename=log_file_name,\n",
        "                        level=logging.INFO,\n",
        "                        format='%(asctime)s [%(levelname)s]: %(message)s',\n",
        "                        datefmt='%Y-%m-%d %H:%M:%S')\n",
        "    return log_file_name\n",
        "\n",
        "# Initialize logger and get the log file name\n",
        "log_file_name = init_logger()\n",
        "print(f\"Logging to file: {log_file_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EvvPIqdwyeS"
      },
      "source": [
        "Preparation of Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDME6gFxwyeT"
      },
      "outputs": [],
      "source": [
        "class WikiText2VocabBuilder:\n",
        "    def _new_index(self):\n",
        "        return len(self.word2index)\n",
        "\n",
        "    def __init__(self, corpus_path, min_freq):\n",
        "        self.corpus_path = corpus_path\n",
        "        self.min_freq = min_freq\n",
        "        self.word2index = defaultdict(self._new_index)\n",
        "        self.index2word = {}\n",
        "        self.word_freqs = Counter()\n",
        "        self.cleaned_sentences = []\n",
        "\n",
        "        self.START_TOKEN = \"<s>\"\n",
        "        self.END_TOKEN = \"</s>\"\n",
        "        self.UNK_TOKEN = \"<unk>\"\n",
        "        self.hyphentoken = \"hyphentoken\"\n",
        "        self.numericalcommatoken = \"numericalcommatoken\"\n",
        "\n",
        "        self._initialize_special_tokens()\n",
        "        self._load_and_preprocess()\n",
        "\n",
        "    def _initialize_special_tokens(self):\n",
        "        self.word2index[self.START_TOKEN] = 0\n",
        "        self.word2index[self.END_TOKEN] = 1\n",
        "        self.word2index[self.UNK_TOKEN] = 2\n",
        "        self.word2index[self.hyphentoken] = 3\n",
        "        self.word2index[self.numericalcommatoken] = 4\n",
        "\n",
        "        self.index2word[0] = self.START_TOKEN\n",
        "        self.index2word[1] = self.END_TOKEN\n",
        "        self.index2word[2] = self.UNK_TOKEN\n",
        "        self.index2word[3] = self.hyphentoken\n",
        "        self.index2word[4] = self.numericalcommatoken\n",
        "\n",
        "\n",
        "    def clean_and_tokenize(self, corpus):\n",
        "        corpus = corpus.lower()  # Convert to lowercase\n",
        "        sentences = sent_tokenize(corpus)\n",
        "        cleaned_sentences = []\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()  # Remove unnecessary whitespaces\n",
        "            if not (sentence.startswith('=') or sentence.endswith('=')):  # Exclude headers\n",
        "                sentence = sentence.replace('<unk>', 'unknowntoken')\n",
        "                sentence = sentence.replace('@-@', 'hyphentoken')\n",
        "                sentence = sentence.replace('@,@', 'numericalcommatoken')\n",
        "                cleaned_sentences.append(sentence)\n",
        "        return cleaned_sentences\n",
        "\n",
        "    def _load_and_preprocess(self):\n",
        "        with open(self.corpus_path, 'r') as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Split the text into sentences using NLTK\n",
        "        self.cleaned_sentences = self.clean_and_tokenize(corpus)\n",
        "        # Count word frequencies\n",
        "        for sentence in self.cleaned_sentences:\n",
        "            words = word_tokenize(sentence)\n",
        "            for word in words:\n",
        "                word = word.lower()\n",
        "                self.word_freqs[word] += 1\n",
        "\n",
        "        # Build the vocabulary using only words that meet the frequency threshold\n",
        "        for word, freq in self.word_freqs.items():\n",
        "            if freq >= self.min_freq:\n",
        "                index = self.word2index[word]\n",
        "                self.index2word[index] = word\n",
        "\n",
        "    def vocab_size(self):\n",
        "        return len(self.word2index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i154_pCSwyeU"
      },
      "outputs": [],
      "source": [
        "class WikiText2Dataset(Dataset):\n",
        "    def __init__(self, preprocessor, context_size):\n",
        "        super(WikiText2Dataset, self).__init__()\n",
        "\n",
        "        self.context_size = context_size\n",
        "\n",
        "        # We already have cleaned sentences in the preprocessor\n",
        "        self.sentences = preprocessor.cleaned_sentences\n",
        "        self.word2index = preprocessor.word2index\n",
        "        self.index2word = preprocessor.index2word\n",
        "        self.word_freqs = preprocessor.word_freqs\n",
        "        self.START_TOKEN = preprocessor.START_TOKEN\n",
        "        self.END_TOKEN = preprocessor.END_TOKEN\n",
        "        self.UNK_TOKEN = preprocessor.UNK_TOKEN\n",
        "        self.hyphentoken = preprocessor.hyphentoken\n",
        "        self.numericalcommatoken = preprocessor.numericalcommatoken\n",
        "\n",
        "\n",
        "        self.X, self.Y = self._build_dataset()\n",
        "\n",
        "    def _build_dataset(self):\n",
        "        X, Y = [], []\n",
        "\n",
        "        for sentence in self.sentences:\n",
        "            words = word_tokenize(sentence)\n",
        "            if not words:\n",
        "                continue\n",
        "            if words[-1] in string.punctuation:\n",
        "                words[-1] = self.END_TOKEN\n",
        "            else:\n",
        "                words.append(self.END_TOKEN)\n",
        "\n",
        "            context = [0] * self.context_size\n",
        "            for i, word in enumerate(words):\n",
        "\n",
        "                if word in self.word2index and word not in ['unknowntoken', 'hyphentoken', 'numericalcommatoken']:\n",
        "                    index = self.word2index[word]\n",
        "                elif word == 'unknowntoken':\n",
        "                    index = self.word2index[self.UNK_TOKEN]\n",
        "                elif word == 'hyphentoken':\n",
        "                    index = self.word2index[self.hyphentoken]\n",
        "                elif word == 'numericalcommatoken':\n",
        "                   index = self.word2index[self.numericalcommatoken]\n",
        "                else:\n",
        "                    index = self.word2index[self.UNK_TOKEN]\n",
        "                X.append(context)\n",
        "                Y.append(index)\n",
        "                context = context[1:] + [index]\n",
        "\n",
        "        return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.Y[index]\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        for _ in range(num_samples):\n",
        "            idx = torch.randint(0, len(self), (1,)).item()\n",
        "            context, target = self.X[idx], self.Y[idx]\n",
        "            context_words = [self.index2word[i.item()] for i in context]\n",
        "            target_word = self.index2word[target.item()]\n",
        "            print(\" \".join(context_words), \"------>\", target_word)\n",
        "\n",
        "    def get_context_size(self):\n",
        "        return self.context_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le0Owij-wyeU"
      },
      "outputs": [],
      "source": [
        "train_vocabBuilder =  WikiText2VocabBuilder(corpus_path = training_data_corpus_path, min_freq=min_freq)\n",
        "\n",
        "train_dataset = WikiText2Dataset(train_vocabBuilder, context_size = context_size)\n",
        "valid_dataset = WikiText2Dataset(train_vocabBuilder, context_size = context_size)\n",
        "test_dataset = WikiText2Dataset(train_vocabBuilder, context_size = context_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aj2RZNfwyeV",
        "outputId": "d732448f-face-41a4-b171-dd2aeaa5ea8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of context-target pairs in the test corpus: 1857512\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "test_dataset_size_msg = f\"Number of context-target pairs in the test corpus: {len(test_dataset)}\"\n",
        "print(test_dataset_size_msg)\n",
        "logging.info(test_dataset_size_msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's architecture begins with an embedding layer that transforms input word indices into dense vector representations. The size of this embedding layer is determined by the vocabulary size and the specified embedding dimension. This layer plays a crucial role in capturing semantic information about words.\n",
        "\n",
        "Following the embedding layer, the model features a linear transformation, embeddingToHiddenLayer, which maps the flattened embedding output to the hidden layer size. This transformation is applied to the input before it is passed through the hidden layers, serving as a bridge between the embedding and the hidden layers.\n",
        "\n",
        "The core of the model consists of a series of hidden layers, each comprising a linear layer followed by layer normalization and a ReLU activation function. These hidden layers are encapsulated within a ModuleList, allowing for a dynamic number of layers based on the numberOfLayers parameter. The use of layer normalization stabilizes the learning process, and ReLU activation introduces non-linearity, enabling the model to learn complex patterns in the data.\n",
        "\n",
        "A key feature of the model is the incorporation of residual connections. Each hidden layer's output is added to its input (residual connection), enhancing gradient flow through the network and mitigating the vanishing gradient problem in deeper architectures.\n",
        "\n",
        "The final component of the model is the output layer, a linear transformation (hiddenToOutputLayer) that maps the output of the last hidden layer to the vocabulary size. This layer produces the logits for each word in the vocabulary.\n",
        "\n",
        "The model employs Kaiming (He) initialization for the embedding and linear layers, optimizing the initial weights for ReLU activation functions. This initialization strategy is known to improve convergence in deep networks.\n",
        "\n",
        "In the forward pass, the model first computes word embeddings, then reshapes the tensor before applying the initial linear transformation. The reshaping flattens the context window and embedding dimensions into a single dimension, preparing the data for linear transformation. The data then sequentially passes through the hidden layers, with residual connections applied at each step. Finally, the output layer generates log probabilities using log softmax, making the model suitable for tasks like next-word prediction.\n",
        "\n",
        "The num_parameters method computes the total number of trainable parameters in the model, providing insight into the model's complexity and capacity."
      ],
      "metadata": {
        "id": "yay29H4-9x1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7Bn636OwyeV"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, context_size, numberOfLayers):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.context_size = context_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.numberOfLayers = numberOfLayers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        self.ReLU = nn.ReLU()  # Defining a single instance of ReLU to reuse\n",
        "\n",
        "\n",
        "        self.embeddingToHiddenLayer = nn.Linear((self.context_size) * self.embed_size, self.hidden_size)\n",
        "        self.hiddenToOutputLayer = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "\n",
        "        # Hidden layers weights, bias, and layer normalization\n",
        "        for i in range(self.numberOfLayers):\n",
        "            linear_layer = nn.Linear((self.context_size) * self.embed_size if i == 0 else self.hidden_size, self.hidden_size)\n",
        "            layer_norm = nn.LayerNorm(self.hidden_size)\n",
        "            ReLULayer = nn.ReLU()\n",
        "\n",
        "\n",
        "            self.hidden_layers.append(nn.Sequential(\n",
        "                linear_layer,\n",
        "                layer_norm,\n",
        "                self.ReLU\n",
        "            ))\n",
        "\n",
        "        self.output_layer = nn.Linear(self.hidden_size, self.vocab_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Xavier initialization for embedding\n",
        "            nn.init.kaiming_normal_(self.embedding.weight)\n",
        "            nn.init.kaiming_normal_(self.embeddingToHiddenLayer.weight)\n",
        "            nn.init.constant_(self.embeddingToHiddenLayer.bias, 0)\n",
        "\n",
        "            nn.init.kaiming_normal_(self.hiddenToOutputLayer.weight)\n",
        "            nn.init.constant_(self.hiddenToOutputLayer.bias, 0)\n",
        "\n",
        "\n",
        "            # Kaiming initialization for linear layers\n",
        "            for hidden_layer in self.hidden_layers:\n",
        "                nn.init.kaiming_normal_(hidden_layer[0].weight)\n",
        "\n",
        "            # Initialize batch normalization layers\n",
        "            for hidden_layer in self.hidden_layers:\n",
        "                nn.init.constant_(hidden_layer[1].weight, 1)\n",
        "                nn.init.constant_(hidden_layer[1].bias, 0)\n",
        "\n",
        "            # Make the output layer less confident\n",
        "            nn.init.constant_(self.output_layer.weight, 0.01)\n",
        "            nn.init.constant_(self.output_layer.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        residual = self.embeddingToHiddenLayer(x)\n",
        "\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x) + residual\n",
        "            residual = x\n",
        "\n",
        "        residual = self.hiddenToOutputLayer(x)\n",
        "        y = self.output_layer(x) + residual\n",
        "        log_probs = F.log_softmax(y, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "    def num_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train_language_model` function takes the model, data loaders for training, validation, and testing, the device for computation, training epochs, learning rate parameters, weight decay, and a patience parameter for early stopping.\n",
        "\n",
        "The training process is iterative, looping over the specified number of epochs. Each epoch involves a forward and backward pass over the training data. The data is loaded in batches from the `train_dataloader`. For each batch, the model performs a forward pass to compute log probabilities of the target words given the contexts. The loss is calculated using negative log-likelihood loss (`nn.NLLLoss`), which is appropriate for the log probabilities output by the model.\n",
        "\n",
        "After computing the loss, a backward pass is performed to compute gradients, which are then used to update the model's parameters via the AdamW optimizer. AdamW is an optimizer with weight decay regularization, suitable for this kind of task. The learning rate is adjusted at each epoch using an exponential decay schedule, calculated based on the initial and final learning rates and the total number of epochs.\n",
        "\n",
        "The function includes an internal `evaluate` method, used to compute the model's performance on the validation and test datasets. This method calculates the total loss over the dataset and returns the perplexity, a common metric in language modeling that measures how well the model predicts a sample.\n",
        "\n",
        "Perplexity is calculated for both the training and validation datasets at the end of each epoch. The training process includes a mechanism for early stopping, which halts training if the validation perplexity does not improve for a number of epochs specified by the `patience` parameter. This approach helps prevent overfitting.\n",
        "\n",
        "After completing the training epochs (or stopping early), the function evaluates the model on the test dataset to compute the test perplexity. This metric provides an assessment of the model's performance on unseen data.\n",
        "\n",
        "The function returns a collection of metrics for each epoch: learning rates, training losses, training perplexities, validation perplexities, and the final test perplexity. These outputs are useful for analyzing the model's learning behavior and overall performance."
      ],
      "metadata": {
        "id": "avum5Q5m-8IX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnS4Z0ShwyeW"
      },
      "outputs": [],
      "source": [
        "def train_language_model(model, train_dataloader, valid_dataloader, test_dataloader, device, total_epochs, start_lr, end_lr, weight_decay_value, patience):\n",
        "\n",
        "    def evaluate(model, dataloader, loss_function, device):\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        total_tokens = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for contexts, targets in dataloader:\n",
        "               contexts, targets = contexts.to(device), targets.to(device)\n",
        "               log_probs = model(contexts)\n",
        "               loss = loss_function(log_probs, targets)\n",
        "               total_loss += loss.item() * targets.size(0)\n",
        "               total_tokens += targets.size(0)\n",
        "\n",
        "        model.train()\n",
        "        return math.exp(total_loss / total_tokens)\n",
        "\n",
        "    \"\"\"Computes the learning rate for the current epoch based on exponential decay.\"\"\"\n",
        "\n",
        "    loss_function = nn.NLLLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=start_lr, weight_decay=weight_decay_value)\n",
        "    scheduler = ExponentialLR(optimizer, gamma=(end_lr/start_lr)**(1/total_epochs))\n",
        "\n",
        "    # Lists to store learning rate, loss, and perplexity values\n",
        "    learning_rates = []\n",
        "    losses = []\n",
        "    train_perplexities = []\n",
        "    valid_perplexities = []\n",
        "    test_perplexities = []\n",
        "\n",
        "    best_valid_perplexity = float('inf')\n",
        "    no_improvement_counter = 0\n",
        "\n",
        "    for epoch in range(total_epochs):\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for i, (contexts, targets) in enumerate(train_dataloader):\n",
        "            # Update learning rate according to the custom schedule\n",
        "\n",
        "            # Move the data to the GPU\n",
        "            contexts, targets = contexts.to(device), targets.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            log_probs = model(contexts)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_function(log_probs, targets)\n",
        "\n",
        "            # Accumulate total loss and total tokens\n",
        "            total_loss += loss.item() * targets.size(0)\n",
        "            total_tokens += targets.size(0)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "        # Check for potential overflow in exponentiation\n",
        "        exponent = total_loss / total_tokens\n",
        "        if exponent > math.log(sys.float_info.max):\n",
        "            overflow_msg = f\"Potential overflow detected at epoch {epoch + 1}. Stopping training for this hyperparameter set.\"\n",
        "            logging.info(overflow_msg)\n",
        "            return None, None, [np.inf], [np.inf], np.inf  # Return inf for all metrics\n",
        "\n",
        "        train_perplexity = math.exp(exponent)\n",
        "\n",
        "        # Store values\n",
        "        losses.append(total_loss / total_tokens)\n",
        "        train_perplexities.append(train_perplexity)\n",
        "\n",
        "        learning_rate = optimizer.param_groups[0]['lr']\n",
        "        learning_rates.append(learning_rate)\n",
        "\n",
        "        # Evaluate on the validation set\n",
        "        valid_perplexity = evaluate(model, valid_dataloader, loss_function, device)\n",
        "\n",
        "        # Check if the validation perplexity has improved\n",
        "        if valid_perplexity < best_valid_perplexity:\n",
        "            best_valid_perplexity = valid_perplexity\n",
        "            no_improvement_counter = 0\n",
        "        else:\n",
        "            no_improvement_counter += 1\n",
        "        if no_improvement_counter >= patience:\n",
        "            early_stopping_msg = f\"Early stopping triggered after {epoch + 1} epochs.\"\n",
        "            logging.info(early_stopping_msg)\n",
        "            print(early_stopping_msg)\n",
        "            break\n",
        "\n",
        "        valid_perplexities.append(valid_perplexity)\n",
        "\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step()\n",
        "        stat_message = f\"Epoch {epoch+1}/{total_epochs} - Training Loss: {total_loss / total_tokens:.4f} - Training Perplexity: {train_perplexities[-1]:.4f} - Validation Perplexity: {valid_perplexities[-1]:.4f} - Learning rate: {learning_rate:.10f}\"\n",
        "        logging.info(stat_message)\n",
        "        print(stat_message)\n",
        "\n",
        "\n",
        "        # Reset total loss and total tokens for the next epoch\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "    test_perplexity = evaluate(model, test_dataloader, loss_function, device)\n",
        "    test_perpl_message = f\"Test Perplexity: {test_perplexity:.4f}\"\n",
        "    logging.info(test_perpl_message)\n",
        "    print(test_perpl_message)\n",
        "\n",
        "    return learning_rates, losses, train_perplexities, valid_perplexities, test_perplexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnngpw_twyeW"
      },
      "source": [
        "Now we train the language model on the WikiText-2 dataset. We initialize the model with specified parameters, conduct training, and evaluates performance on training, validation, and test sets. Key metrics like perplexities and losses are logged and reported post-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Jrat66wyeX",
        "outputId": "ff476f72-a716-4185-9d59-37234ebc850e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Used Device: cuda\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Model Parameters: 61699756\n",
            "Epoch 1/15 - Training Loss: 5.7492 - Training Perplexity: 313.9442 - Validation Perplexity: 82.4057 - Learning rate: 0.0007125891\n",
            "Epoch 2/15 - Training Loss: 4.2933 - Training Perplexity: 73.2047 - Validation Perplexity: 21.4043 - Learning rate: 0.0004506159\n",
            "Epoch 3/15 - Training Loss: 3.0995 - Training Perplexity: 22.1863 - Validation Perplexity: 9.9646 - Learning rate: 0.0002849534\n",
            "Epoch 4/15 - Training Loss: 2.3958 - Training Perplexity: 10.9765 - Validation Perplexity: 6.7038 - Learning rate: 0.0001801944\n",
            "Epoch 5/15 - Training Loss: 1.9902 - Training Perplexity: 7.3170 - Validation Perplexity: 5.2946 - Learning rate: 0.0001139485\n",
            "Epoch 6/15 - Training Loss: 1.7383 - Training Perplexity: 5.6878 - Validation Perplexity: 4.5922 - Learning rate: 0.0000720570\n",
            "Epoch 7/15 - Training Loss: 1.5779 - Training Perplexity: 4.8448 - Validation Perplexity: 4.2031 - Learning rate: 0.0000455663\n",
            "Epoch 8/15 - Training Loss: 1.4753 - Training Perplexity: 4.3725 - Validation Perplexity: 3.9806 - Learning rate: 0.0000288145\n",
            "Epoch 9/15 - Training Loss: 1.4099 - Training Perplexity: 4.0956 - Validation Perplexity: 3.8520 - Learning rate: 0.0000182212\n",
            "Epoch 10/15 - Training Loss: 1.3681 - Training Perplexity: 3.9280 - Validation Perplexity: 3.7747 - Learning rate: 0.0000115225\n",
            "Epoch 11/15 - Training Loss: 1.3415 - Training Perplexity: 3.8249 - Validation Perplexity: 3.7278 - Learning rate: 0.0000072864\n",
            "Epoch 12/15 - Training Loss: 1.3246 - Training Perplexity: 3.7606 - Validation Perplexity: 3.6992 - Learning rate: 0.0000046077\n",
            "Epoch 13/15 - Training Loss: 1.3138 - Training Perplexity: 3.7204 - Validation Perplexity: 3.6813 - Learning rate: 0.0000029137\n",
            "Epoch 14/15 - Training Loss: 1.3070 - Training Perplexity: 3.6950 - Validation Perplexity: 3.6702 - Learning rate: 0.0000018425\n",
            "Epoch 15/15 - Training Loss: 1.3026 - Training Perplexity: 3.6789 - Validation Perplexity: 3.6632 - Learning rate: 0.0000011652\n",
            "Test Perplexity: 3.6632\n",
            "Validation Perplexity: 3.6632, Training Loss: 1.3026, Training Perplexity: 3.6789\n"
          ]
        }
      ],
      "source": [
        "vocab_size = train_vocabBuilder.vocab_size() # The size of the vocabulary\n",
        "\n",
        "# Initialize device based on the configuration and availability\n",
        "device = torch.device(config_device if torch.cuda.is_available() else \"cpu\")\n",
        "device_msg = f\"Used Device: {device}\"\n",
        "logging.info(device_msg)\n",
        "print(device_msg)\n",
        "valid_perplexities_dict = {}\n",
        "test_perplexity_dict = {}\n",
        "\n",
        "np.random.seed(423455335)\n",
        "# Set the seed for generating random numbers\n",
        "torch.manual_seed(423455335)\n",
        "# Set the seed for generating random numbers for CUDA when using the GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Initialize the dictionary that will hold all the data\n",
        "results_dict = {}\n",
        "\n",
        "model = LanguageModel(vocab_size, embed_size, hidden_size, context_size, numberOfLayers)\n",
        "model = model.to(device)\n",
        "\n",
        "model_param_msg = f\"Number of Model Parameters: {model.num_parameters()}\"\n",
        "print(model_param_msg)\n",
        "logging.info(model_param_msg)\n",
        "learning_rates, losses, train_perplexities, valid_perplexities, test_perplexity = train_language_model(model, train_dataloader, valid_dataloader, test_dataloader, device, total_epochs, lr_start, lr_end, weight_decay, patience)\n",
        "\n",
        "iter_dict = {\n",
        "    'learning_rates': learning_rates,\n",
        "    'losses': losses,\n",
        "    'train_perplexities': train_perplexities,\n",
        "    'valid_perplexities': valid_perplexities,\n",
        "    'test_perplexity': test_perplexity\n",
        "}\n",
        "\n",
        "result_msg = f\"Validation Perplexity: {valid_perplexities[-1]:.4f}, Training Loss: {losses[-1]:.4f}, Training Perplexity: {train_perplexities[-1]:.4f}, Test Perplexity: {test_perplexity:.4f}\"\n",
        "print(result_msg)\n",
        "logging.info(result_msg)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save the weights so that we can analyze the embedding space of the trained model in Experiment 1."
      ],
      "metadata": {
        "id": "GZBWcdNE_ONS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kANMvD_bwyeX",
        "outputId": "ab215d82-e1bf-4f1f-f0b6-4f97cf20e9e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved at ./testModel_weights.pth\n"
          ]
        }
      ],
      "source": [
        "model_save_path = \"./testModel_weights.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved at {model_save_path}\")\n",
        "logging.info(f\"Model saved at {model_save_path}\")\n",
        "\n",
        "\n",
        "json_filename = \"finalRun.json\"\n",
        "with open(json_filename, 'w') as f:\n",
        "    json.dump(iter_dict, f, indent=4)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "transformers",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}