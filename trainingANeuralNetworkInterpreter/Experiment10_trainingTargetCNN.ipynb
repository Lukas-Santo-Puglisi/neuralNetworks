{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this script, I construct the target convolutional neural network and train it on the MNIST dataset. I decided for an architecture with large kernels, loss stride and high padding. The reason for this choice is that the activation maps before the second covolutional layer are comparatavely bigger. This allows for an easier study of interpretability in Experiment 11.\n",
        "\n",
        "The architecture is as follows:\n",
        "\n",
        "1. **Conv1**: 1→8 channels, 9x9 kernel, stride 1, padding 4, ReLU, MaxPool (2x2), He Initialization.\n",
        "2. **Conv2**: 8→16 channels, 9x9 kernel, stride 1, padding 4, ReLU, MaxPool (2x2), He Initialization.\n",
        "3. **FC Layer**: 16 * 7 * 7 input, 10 output (MNIST classes), He Initialization with ReLU.\n"
      ],
      "metadata": {
        "id": "PG7vgoVKTa4b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-wgKOhp0vEg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Initialize convolutional layer 1 with He Initialization\n",
        "        # Increased kernel size to 9 and number of filters remains 8\n",
        "        self.conv1 = self._init_conv_layer(1, 8, 9, 1, 4, 'he')  # Padding adjusted to 4\n",
        "\n",
        "        # Initialize convolutional layer 2 with He Initialization\n",
        "        # Increased kernel size to 9 and number of filters remains 16\n",
        "        self.conv2 = self._init_conv_layer(8, 16, 9, 1, 4, 'he')  # Padding adjusted to 4\n",
        "\n",
        "        # Initialize fully connected output layer\n",
        "        # Input dimension remains the same\n",
        "        self.out = nn.Linear(16 * 7 * 7, 10)\n",
        "        nn.init.kaiming_normal_(self.out.weight, nonlinearity='relu')\n",
        "\n",
        "    def _init_conv_layer(self, in_channels, out_channels, kernel_size, stride, padding, init_method):\n",
        "        \"\"\"Initializes a convolutional layer followed by a ReLU activation and MaxPool.\"\"\"\n",
        "        layer = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                padding=padding\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        if init_method == 'he':\n",
        "            nn.init.kaiming_normal_(layer[0].weight, nonlinearity='relu')\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        # Flatten the output for the fully connected layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        output = self.out(x)\n",
        "        return output, x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we instantiate the neural network, train it on the MNIST data set and subsequently evaluate it."
      ],
      "metadata": {
        "id": "gE4pRwF7OS0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Data Preprocessing and Loading\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        outputs, _ = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] completed.')\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs, _ = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f'Test Accuracy of the model: {test_accuracy}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cEkjGiV1g8A",
        "outputId": "a35c4cd9-d560-4608-e2bf-23dee9d1d158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 245956277.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 103446365.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 18130056.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22229321.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch [1/10] completed.\n",
            "Epoch [2/10] completed.\n",
            "Epoch [3/10] completed.\n",
            "Epoch [4/10] completed.\n",
            "Epoch [5/10] completed.\n",
            "Epoch [6/10] completed.\n",
            "Epoch [7/10] completed.\n",
            "Epoch [8/10] completed.\n",
            "Epoch [9/10] completed.\n",
            "Epoch [10/10] completed.\n",
            "Test Accuracy of the model: 98.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we save the weights so that we can reuse the target CNN for later analysis."
      ],
      "metadata": {
        "id": "AEtEKICmOmxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"mnistTwoLayersFilterSize9And16Filters.pth\")\n",
        "from google.colab import files\n",
        "files.download('mnistTwoLayersFilterSize9And16Filters.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yypeaYko94z1",
        "outputId": "75a87616-6706-4bcf-dba6-2a7a41329c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c20429d7-3134-4c40-8ce9-f788f00bd2b5\", \"mnistTwoLayersFilterSize9And16Filters.pth\", 78644)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
